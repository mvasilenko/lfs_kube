On Google Cloud, make a 2 vcpu, 7.5 GB n1-standard-2 with Ubuntu 16.04 LTS

`gcloud config set project lfs258-220911`

`gcloud config set compute/zone "europe-north1-c"`

`gcloud compute instances create lfs258-worker1 --zone europe-north1-a --machine-type n1-standard-2 --preemptible --image ubuntu-1604-xenial-v20181023 --image-project ubuntu-os-cloud`

`gcloud compute instances create lfs258-worker1 --zone europe-north1-a --machine-type n1-standard-2 --preemptible --image ubuntu-1604-xenial-v20181023 --image-project ubuntu-os-cloud`

`gcloud compute instances list`

list firewall rules
`gcloud compute firewall-rules list`

allow traffic between instances

`gcloud compute firewall-rules create calico --allow 4 --source-ranges 10.166.0.0/20`

`sudo -s`

add docker repo key

`curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -`

add docker repo

`sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"`

add kubernetes repo key

`curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -`

```
cat > /etc/apt/sources.list.d/kubernetes.list <<EOF
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
```

update packages list

`apt-get -y update`

INSTALL docker and kubernetes binaries

`apt-get -y install docker-ce kubeadm kubectl kubelet`

GET CALICO CONFIG

`wget https://goo.gl/eWLkzb -O calico.yaml`

INITIALIZE CLUSTER

`kubeadm init --pod-network-cidr 192.168.0.0/16`

GO BACK TO NON-ROOT USER AND COPY THE KEYS,ETC FROM MAIN CONFIG TO A USER

`exit`

`mkdir -p $HOME/.kube`

`sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config`

`sudo chown $(id -u):$(id -g) $HOME/.kube/config`

THEN ADD THE CNI CONFIGURATION (MISSING FROM MANUAL)

`mkdir -p /etc/cni/net.d`

```
cat > /etc/cni/net.d/10-flannel.conflist <<EOF
{
    "name": "cbr0",
    "plugins": [ 
        {
            "type": "flannel",
            "delegate": {
                "hairpinMode": true,
                "isDefaultGateway": true
            }
        },
        {
            "type": "portmap",
            "capabilities": {
                "portMappings": true
            }
        }
    ]
}
EOF
```
  
COPY THE CALICO CONFIG FROM THE ROOT USER TO THE LOCAL USER

`sudo cp /root/calico.yaml .`

INITIALIZE THE CALICO POD NETWORKING STUFF

`kubectl apply -f calico.yaml`

CHECK TO MAKE SURE THE TUNNEL IS THERE

`ip a`

MAKE SURE THE NODE IS IN READY MODE

`kubectl get node`

GET MORE INFO ABOUT THE NODE

`kubectl describe node <NAME OF NODE>`

REMOVE THE 'TAINT' FROM THE MASTER SERVER SO IT CAN RUN NON-INFRA PODS

`kubectl taint nodes --all node-role.kubernetes.io/master-`

SHOW THE TAINTS REMOVED

`kubectl describe node <NAME OF NODE> | grep -t taint`

ADD BASH AUTOCOMPLETION

`source <(kubectl completion bash)`

ADD BASH AUTOCOMPLETION FOR NEXT LOGIN

`echo "source <(kubectl completion bash)" >> ~/.bashrc`

Repeat the steps above for worker node

go to master, list available tokens, generate new if needed

`kubeadm token list`

`kubeadm token create`


generate Discovery Token CA Cert Hash

```
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
```

get master internal ipv4 address

`ip addr show ens4 | grep inet`

go to worker node, join cluster

```
kubeadm join \
--token token_value \
master.ip:6443 \
--discovery-token-ca-cert-hash \
sha256:discovery_token_ca_cert_hash_value
```

go to master, check nodes status, worker will join in a few minutes

`kubectl get nodes`


